{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8579457",
   "metadata": {},
   "source": [
    "### Data Warehousing Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b172d",
   "metadata": {},
   "source": [
    "TOPIC: Data Warehousing Fundamentals\n",
    "   1. Design a data warehouse schema for a retail company that includes dimension tables for products, customers, and time. Implement the schema using a relational database management system (RDBMS) of your choice.\n",
    "   2. Create a fact table that captures sales data, including product ID, customer ID, date, and sales amount. Populate the fact table with sample data.\n",
    "   3. Write SQL queries to retrieve sales data from the data warehouse, including aggregations and filtering based on different dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767dada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Design a data warehouse schema for a retail company that includes dimension tables for products, customers, and time. Implement the schema using a relational database management system (RDBMS) of your choice.\n",
    "\n",
    "# Designing a data warehouse schema involves creating dimension tables and a fact table to store data related to a retail company. In this example, we'll create three dimension tables for products, customers, and time, and a fact table to store sales data.\n",
    "\n",
    "# Data Warehouse Schema:\n",
    "\n",
    "# Products Dimension Table:\n",
    "\n",
    "# product_id (Primary Key)\n",
    "# product_name\n",
    "# category\n",
    "# price\n",
    "# ...\n",
    "# Customers Dimension Table:\n",
    "\n",
    "# customer_id (Primary Key)\n",
    "# customer_name\n",
    "# city\n",
    "# state\n",
    "# ...\n",
    "# Time Dimension Table:\n",
    "\n",
    "# date_id (Primary Key)\n",
    "# date\n",
    "# day\n",
    "# month\n",
    "# year\n",
    "# ...\n",
    "# Sales Fact Table:\n",
    "\n",
    "# sale_id (Primary Key)\n",
    "# product_id (Foreign Key referencing Products Dimension Table)\n",
    "# customer_id (Foreign Key referencing Customers Dimension Table)\n",
    "# date_id (Foreign Key referencing Time Dimension Table)\n",
    "# quantity\n",
    "# total_amount\n",
    "# ...\n",
    "\n",
    "\n",
    "-- Create Products Dimension Table\n",
    "CREATE TABLE products (\n",
    "    product_id INT PRIMARY KEY,\n",
    "    product_name VARCHAR(100),\n",
    "    category VARCHAR(50),\n",
    "    price DECIMAL(10, 2)\n",
    "    -- Add more product attributes as needed\n",
    ");\n",
    "\n",
    "-- Create Customers Dimension Table\n",
    "CREATE TABLE customers (\n",
    "    customer_id INT PRIMARY KEY,\n",
    "    customer_name VARCHAR(100),\n",
    "    city VARCHAR(50),\n",
    "    state VARCHAR(50)\n",
    "    -- Add more customer attributes as needed\n",
    ");\n",
    "\n",
    "-- Create Time Dimension Table\n",
    "CREATE TABLE time (\n",
    "    date_id INT PRIMARY KEY,\n",
    "    date DATE,\n",
    "    day INT,\n",
    "    month INT,\n",
    "    year INT\n",
    "    -- Add more time attributes as needed\n",
    ");\n",
    "\n",
    "-- Create Sales Fact Table\n",
    "CREATE TABLE sales (\n",
    "    sale_id INT PRIMARY KEY,\n",
    "    product_id INT,\n",
    "    customer_id INT,\n",
    "    date_id INT,\n",
    "    quantity INT,\n",
    "    total_amount DECIMAL(10, 2)\n",
    "    -- Add more sales-related attributes as needed\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6513bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a fact table that captures sales data, including product ID, customer ID, date, and sales amount. Populate the fact table with sample data.\n",
    "\n",
    "-- Create Sales Fact Table\n",
    "CREATE TABLE sales (\n",
    "    sale_id INT PRIMARY KEY,\n",
    "    product_id INT,\n",
    "    customer_id INT,\n",
    "    date_id INT,\n",
    "    quantity INT,\n",
    "    total_amount DECIMAL(10, 2)\n",
    ");\n",
    "\n",
    "-- Insert Sample Data into Sales Fact Table\n",
    "INSERT INTO sales (sale_id, product_id, customer_id, date_id, quantity, total_amount)\n",
    "VALUES\n",
    "    (1, 101, 201, 301, 5, 250.00),\n",
    "    (2, 102, 202, 302, 3, 180.00),\n",
    "    (3, 103, 201, 303, 2, 100.00),\n",
    "    (4, 101, 203, 304, 4, 200.00),\n",
    "    (5, 102, 204, 305, 1, 60.00),\n",
    "    -- Add more sample data here\n",
    "    (6, 104, 202, 306, 2, 120.00),\n",
    "    (7, 103, 204, 307, 3, 150.00),\n",
    "    (8, 101, 201, 308, 6, 300.00),\n",
    "    (9, 102, 203, 309, 4, 240.00),\n",
    "    (10, 104, 205, 310, 2, 120.00);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04fd6c0",
   "metadata": {},
   "source": [
    "   3. Write SQL queries to retrieve sales data from the data warehouse, including aggregations and filtering based on different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Retrieve Total Sales Amount for Each Product:\n",
    "\n",
    "SELECT\n",
    "    product_id,\n",
    "    SUM(total_amount) AS total_sales_amount\n",
    "FROM\n",
    "    sales\n",
    "GROUP BY\n",
    "    product_id;\n",
    "\n",
    "# 2. Retrieve Total Sales Amount for Each Customer:\n",
    "\n",
    "SELECT\n",
    "    customer_id,\n",
    "    SUM(total_amount) AS total_sales_amount\n",
    "FROM\n",
    "    sales\n",
    "GROUP BY\n",
    "    customer_id;\n",
    "\n",
    "# 3. Retrieve Total Sales Amount for Each Month:\n",
    "\n",
    "SELECT\n",
    "    t.year,\n",
    "    t.month,\n",
    "    SUM(total_amount) AS total_sales_amount\n",
    "FROM\n",
    "    sales s\n",
    "INNER JOIN\n",
    "    time t ON s.date_id = t.date_id\n",
    "GROUP BY\n",
    "    t.year,\n",
    "    t.month;\n",
    "\n",
    "# 4. Retrieve Sales Amount for a Specific Product in a Specific Month:\n",
    "\n",
    "SELECT\n",
    "    p.product_name,\n",
    "    t.year,\n",
    "    t.month,\n",
    "    SUM(s.total_amount) AS total_sales_amount\n",
    "FROM\n",
    "    sales s\n",
    "INNER JOIN\n",
    "    products p ON s.product_id = p.product_id\n",
    "INNER JOIN\n",
    "    time t ON s.date_id = t.date_id\n",
    "WHERE\n",
    "    p.product_name = 'Product1'\n",
    "    AND t.year = 2023\n",
    "    AND t.month = 7\n",
    "GROUP BY\n",
    "    p.product_name,\n",
    "    t.year,\n",
    "    t.month;\n",
    "\n",
    "# 5. Retrieve Top N Products with the Highest Sales Amount:\n",
    "\n",
    "SELECT\n",
    "    p.product_name,\n",
    "    SUM(s.total_amount) AS total_sales_amount\n",
    "FROM\n",
    "    sales s\n",
    "INNER JOIN\n",
    "    products p ON s.product_id = p.product_id\n",
    "GROUP BY\n",
    "    p.product_name\n",
    "ORDER BY\n",
    "    total_sales_amount DESC\n",
    "LIMIT N; -- Replace N with the desired number of top products to retrieve\n",
    "\n",
    "# 6. Retrieve Customers with the Highest Total Purchases:\n",
    "\n",
    "SELECT\n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    SUM(s.total_amount) AS total_purchases\n",
    "FROM\n",
    "    sales s\n",
    "INNER JOIN\n",
    "    customers c ON s.customer_id = c.customer_id\n",
    "GROUP BY\n",
    "    c.customer_id,\n",
    "    c.customer_name\n",
    "ORDER BY\n",
    "    total_purchases DESC;\n",
    "\n",
    "# 7. Retrieve Total Sales Amount for Each Category of Products:\n",
    "   SELECT\n",
    "    p.category,\n",
    "    SUM(s.total_amount) AS total_sales_amount\n",
    "FROM\n",
    "    sales s\n",
    "INNER JOIN\n",
    "    products p ON s.product_id = p.product_id\n",
    "GROUP BY\n",
    "    p.category;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f56a24",
   "metadata": {},
   "source": [
    "TOPIC: ETL and Data Integration\n",
    "  1. Design an ETL process using a programming language (e.g., Python) to extract data from a source system (e.g., CSV files), transform it by applying certain business rules or calculations, and load it into a data warehouse.\n",
    "  2. Implement the ETL process by writing code that performs the extraction, transformation, and loading steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e588a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# . Design an ETL process using a programming language (e.g., Python) to extract data from a source system (e.g., CSV files), transform it by applying certain business rules or calculations, and load it into a data warehouse.\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def extract_data_from_csv(csv_file):\n",
    "    # Read data from CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df\n",
    "\n",
    "def transform_data(df):\n",
    "    # Applying transformations (For example, calculating a new column)\n",
    "    df['total_price'] = df['quantity'] * df['unit_price']\n",
    "    return df\n",
    "\n",
    "def load_data_to_database(df, db_file):\n",
    "    # Create a connection to the SQLite database\n",
    "    engine = create_engine(f'sqlite:///{db_file}')\n",
    "\n",
    "    # Store the DataFrame into a database table (replace 'your_table_name' with the desired table name)\n",
    "    df.to_sql('your_table_name', engine, index=False, if_exists='replace')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_csv_file.csv' with the path to your CSV file\n",
    "    csv_file = 'your_csv_file.csv'\n",
    "\n",
    "    # Replace 'your_database_file.db' with the desired name for the SQLite database file\n",
    "    db_file = 'your_database_file.db'\n",
    "\n",
    "    # Step 1: Extract data from CSV\n",
    "    data_df = extract_data_from_csv(csv_file)\n",
    "\n",
    "    # Step 2: Transform data\n",
    "    transformed_df = transform_data(data_df)\n",
    "\n",
    "    # Step 3: Load data into the database\n",
    "    load_data_to_database(transformed_df, db_file)\n",
    "\n",
    "    print(\"ETL process completed successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71165b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2. Implement the ETL process by writing code that performs the extraction, transformation, and loading steps.\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def extract_data_from_csv(csv_file):\n",
    "    # Read data from CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df\n",
    "\n",
    "def transform_data(df):\n",
    "    # Applying transformations (For example, calculating a new column)\n",
    "    df['total_price'] = df['quantity'] * df['unit_price']\n",
    "    return df\n",
    "\n",
    "def load_data_to_database(df, db_file, table_name):\n",
    "    # Create a connection to the SQLite database\n",
    "    engine = create_engine(f'sqlite:///{db_file}')\n",
    "\n",
    "    # Store the DataFrame into a database table\n",
    "    df.to_sql(table_name, engine, index=False, if_exists='replace')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_csv_file.csv' with the path to your CSV file\n",
    "    csv_file = 'your_csv_file.csv'\n",
    "\n",
    "    # Replace 'your_database_file.db' with the desired name for the SQLite database file\n",
    "    db_file = 'your_database_file.db'\n",
    "\n",
    "    # Replace 'your_table_name' with the desired table name\n",
    "    table_name = 'your_table_name'\n",
    "\n",
    "    # Step 1: Extract data from CSV\n",
    "    data_df = extract_data_from_csv(csv_file)\n",
    "\n",
    "    # Step 2: Transform data\n",
    "    transformed_df = transform_data(data_df)\n",
    "\n",
    "    # Step 3: Load data into the database\n",
    "    load_data_to_database(transformed_df, db_file, table_name)\n",
    "\n",
    "    print(\"ETL process completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2026d",
   "metadata": {},
   "source": [
    "TOPIC: Dimensional Modeling and Schemas\n",
    "   1. Design a star schema for a university database, including a fact table for student enrollments and dimension tables for students, courses, and time. Implement the schema using a database of your choice.\n",
    "   2. Write SQL queries to retrieve data from the star schema, including aggregations and joins between the fact table and dimension tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8069e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    1. Design a star schema for a university database, including a fact table for student enrollments and dimension tables for students, courses, and time. Implement the schema using a database of your choice.\n",
    "\n",
    "# Star Schema for University Database:\n",
    "\n",
    "# In this example, we'll design a star schema for a university database to track student enrollments in courses over time.\n",
    "\n",
    "# Fact Table:\n",
    "\n",
    "# Enrollments Fact Table:\n",
    "# enrollment_id (Primary Key)\n",
    "# student_id (Foreign Key referencing Students Dimension Table)\n",
    "# course_id (Foreign Key referencing Courses Dimension Table)\n",
    "# date_id (Foreign Key referencing Time Dimension Table)\n",
    "# enrollment_count (e.g., number of students enrolled in a course on a particular date)\n",
    "# Dimension Tables:\n",
    "\n",
    "# Students Dimension Table:\n",
    "\n",
    "# student_id (Primary Key)\n",
    "# student_name\n",
    "# student_age\n",
    "# student_gender\n",
    "# ...\n",
    "# Courses Dimension Table:\n",
    "\n",
    "# course_id (Primary Key)\n",
    "# course_name\n",
    "# course_department\n",
    "# course_credits\n",
    "# ...\n",
    "# Time Dimension Table:\n",
    "\n",
    "# date_id (Primary Key)\n",
    "# date\n",
    "# day_of_week\n",
    "# month\n",
    "# year\n",
    "# ...\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "def create_tables():\n",
    "    # Connect to the SQLite database (or create one if it doesn't exist)\n",
    "    conn = sqlite3.connect('university.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Create Students Dimension Table\n",
    "    c.execute('''\n",
    "        CREATE TABLE students (\n",
    "            student_id INTEGER PRIMARY KEY,\n",
    "            student_name TEXT,\n",
    "            student_age INTEGER,\n",
    "            student_gender TEXT\n",
    "            -- Add more student attributes as needed\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    # Create Courses Dimension Table\n",
    "    c.execute('''\n",
    "        CREATE TABLE courses (\n",
    "            course_id INTEGER PRIMARY KEY,\n",
    "            course_name TEXT,\n",
    "            course_department TEXT,\n",
    "            course_credits INTEGER\n",
    "            -- Add more course attributes as needed\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    # Create Time Dimension Table\n",
    "    c.execute('''\n",
    "        CREATE TABLE time (\n",
    "            date_id INTEGER PRIMARY KEY,\n",
    "            date TEXT,\n",
    "            day_of_week TEXT,\n",
    "            month INTEGER,\n",
    "            year INTEGER\n",
    "            -- Add more time attributes as needed\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    # Create Enrollments Fact Table\n",
    "    c.execute('''\n",
    "        CREATE TABLE enrollments (\n",
    "            enrollment_id INTEGER PRIMARY KEY,\n",
    "            student_id INTEGER,\n",
    "            course_id INTEGER,\n",
    "            date_id INTEGER,\n",
    "            enrollment_count INTEGER\n",
    "            -- Add more fact attributes as needed\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_tables()\n",
    "    print(\"Star schema tables created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2. Write SQL queries to retrieve data from the star schema, including aggregations and joins between the fact table and dimension tables.\n",
    "\n",
    "# 1. Retrieve Student Enrollment Count for Each Course:\n",
    "\n",
    "SELECT\n",
    "    c.course_name,\n",
    "    SUM(e.enrollment_count) AS total_enrollments\n",
    "FROM\n",
    "    enrollments e\n",
    "INNER JOIN\n",
    "    courses c ON e.course_id = c.course_id\n",
    "GROUP BY\n",
    "    c.course_name;\n",
    "\n",
    "    \n",
    "# 2. Retrieve Student Enrollment Count for Each Course on a Specific Date:\n",
    "\n",
    "SELECT\n",
    "    c.course_name,\n",
    "    t.date,\n",
    "    e.enrollment_count\n",
    "FROM\n",
    "    enrollments e\n",
    "INNER JOIN\n",
    "    courses c ON e.course_id = c.course_id\n",
    "INNER JOIN\n",
    "    time t ON e.date_id = t.date_id\n",
    "WHERE\n",
    "    t.date = '2023-07-15'; -- Replace with the desired date\n",
    "\n",
    "# 3. Retrieve Student Information for Enrollments in a Specific Course:\n",
    "\n",
    "SELECT\n",
    "    s.student_id,\n",
    "    s.student_name,\n",
    "    s.student_age,\n",
    "    s.student_gender,\n",
    "    e.enrollment_count\n",
    "FROM\n",
    "    enrollments e\n",
    "INNER JOIN\n",
    "    students s ON e.student_id = s.student_id\n",
    "WHERE\n",
    "    e.course_id = 101; -- Replace with the desired course_id\n",
    "\n",
    "# 4. Retrieve Student Enrollment Count by Year and Month:\n",
    "\n",
    "SELECT\n",
    "    t.year,\n",
    "    t.month,\n",
    "    SUM(e.enrollment_count) AS total_enrollments\n",
    "FROM\n",
    "    enrollments e\n",
    "INNER JOIN\n",
    "    time t ON e.date_id = t.date_id\n",
    "GROUP BY\n",
    "    t.year,\n",
    "    t.month;\n",
    "\n",
    "# 5. Retrieve Student Enrollment Count for Male and Female Students in Each Course:\n",
    "\n",
    "SELECT\n",
    "    c.course_name,\n",
    "    s.student_gender,\n",
    "    SUM(e.enrollment_count) AS total_enrollments\n",
    "FROM\n",
    "    enrollments e\n",
    "INNER JOIN\n",
    "    courses c ON e.course_id = c.course_id\n",
    "INNER JOIN\n",
    "    students s ON e.student_id = s.student_id\n",
    "GROUP BY\n",
    "    c.course_name,\n",
    "    s.student_gender;\n",
    "\n",
    "\n",
    "# 6. Retrieve Student Enrollment Count for Each Department and Year:\n",
    "\n",
    "SELECT\n",
    "    c.course_department,\n",
    "    t.year,\n",
    "    SUM(e.enrollment_count) AS total_enrollments\n",
    "FROM\n",
    "    enrollments e\n",
    "INNER JOIN\n",
    "    courses c ON e.course_id = c.course_id\n",
    "INNER JOIN\n",
    "    time t ON e.date_id = t.date_id\n",
    "GROUP BY\n",
    "    c.course_department,\n",
    "    t.year;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050fa89",
   "metadata": {},
   "source": [
    "TOPIC: Performance Optimization and Querying\n",
    "    1. Scenario: You need to improve the performance of your data loading process in the data warehouse. Write a Python script that implements the following optimizations:\n",
    "Utilize batch processing techniques to load data in bulk instead of individual row insertion.\n",
    "      b)  Implement multi-threading or multiprocessing to parallelize the data loading process.\n",
    "      c)  Measure the time taken to load a specific amount of data before and after implementing these optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe799e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Utilize batch processing techniques to load data in bulk instead of individual row insertion.\n",
    "\n",
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "def create_tables():\n",
    "    # Database connection and table creation code (same as in the previous example)\n",
    "    # ...\n",
    "\n",
    "def load_data_to_database_bulk(csv_file, db_file, table_name):\n",
    "    # Create a connection to the SQLite database\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Read data from CSV file and store it in a list of tuples\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        data_to_insert = [tuple(row) for row in reader]\n",
    "\n",
    "    # Prepare the SQL INSERT statement\n",
    "    placeholders = ','.join(['?'] * len(data_to_insert[0]))\n",
    "    sql_query = f'INSERT INTO {table_name} VALUES ({placeholders})'\n",
    "\n",
    "    # Use executemany to insert data in bulk\n",
    "    c.executemany(sql_query, data_to_insert)\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_csv_file.csv' with the path to your CSV file\n",
    "    csv_file = 'your_csv_file.csv'\n",
    "\n",
    "    # Replace 'your_database_file.db' with the desired name for the SQLite database file\n",
    "    db_file = 'your_database_file.db'\n",
    "\n",
    "    # Replace 'your_table_name' with the desired table name\n",
    "    table_name = 'your_table_name'\n",
    "\n",
    "    # Step 1: Create tables (same as before)\n",
    "    create_tables()\n",
    "\n",
    "    # Step 2: Load data into the database using batch processing\n",
    "    load_data_to_database_bulk(csv_file, db_file, table_name)\n",
    "\n",
    "    print(\"Data loading using batch processing completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   b)  Implement multi-threading or multiprocessing to parallelize the data loading process.\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import concurrent.futures\n",
    "\n",
    "def create_table():\n",
    "    # Same as before\n",
    "\n",
    "def insert_data_batch(data_batch):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect('example.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Prepare the SQL INSERT statement\n",
    "    sql_query = 'INSERT INTO students (name, age, grade) VALUES (?, ?, ?)'\n",
    "\n",
    "    # Use executemany to insert data in bulk\n",
    "    c.executemany(sql_query, data_batch)\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def load_data_to_database_parallel(data_to_insert, batch_size=1000, num_threads=4):\n",
    "    # Step 1: Create the table (if not exists)\n",
    "    create_table()\n",
    "\n",
    "    # Divide the data into batches\n",
    "    data_batches = [data_to_insert[i:i + batch_size] for i in range(0, len(data_to_insert), batch_size)]\n",
    "\n",
    "    # Step 2: Load data into the database using multi-threading\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        # Map the insert_data_batch function to the data batches in parallel\n",
    "        executor.map(insert_data_batch, data_batches)\n",
    "\n",
    "    print(\"Data loading using multi-threading completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data to be inserted into the database in bulk\n",
    "    data_to_insert = [\n",
    "        ('Alice', 25, 'A'),\n",
    "        ('Bob', 22, 'B'),\n",
    "        ('Charlie', 21, 'C'),\n",
    "        # Add more data as needed\n",
    "    ]\n",
    "\n",
    "    # Step 3: Load data into the database using multi-threading\n",
    "    load_data_to_database_parallel(data_to_insert, batch_size=1000, num_threads=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b11936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     c)  Measure the time taken to load a specific amount of data before and after implementing these optimizations.\n",
    "\n",
    "import sqlite3\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "# Rest of the functions and database setup are the same as before\n",
    "\n",
    "def load_data_to_database_bulk(data):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect('example.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Prepare the SQL INSERT statement\n",
    "    sql_query = 'INSERT INTO students (name, age, grade) VALUES (?, ?, ?)'\n",
    "\n",
    "    # Use executemany to insert data in bulk\n",
    "    c.executemany(sql_query, data)\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def load_data_to_database_parallel(data_to_insert, batch_size=1000, num_threads=4):\n",
    "    # Step 1: Create the table (if not exists)\n",
    "    create_table()\n",
    "\n",
    "    # Divide the data into batches\n",
    "    data_batches = [data_to_insert[i:i + batch_size] for i in range(0, len(data_to_insert), batch_size)]\n",
    "\n",
    "    # Measure time before data loading\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 2: Load data into the database using multi-threading\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        # Map the insert_data_batch function to the data batches in parallel\n",
    "        executor.map(insert_data_batch, data_batches)\n",
    "\n",
    "    # Measure time after data loading\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Data loading using multi-threading completed successfully in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data to be inserted into the database in bulk\n",
    "    data_to_insert = [\n",
    "        ('Alice', 25, 'A'),\n",
    "        ('Bob', 22, 'B'),\n",
    "        ('Charlie', 21, 'C'),\n",
    "        # Add more data as needed\n",
    "    ]\n",
    "\n",
    "    # Load data without optimizations\n",
    "    print(\"Loading data without optimizations:\")\n",
    "    load_data_to_database_bulk(data_to_insert)\n",
    "\n",
    "    # Load data with multi-threading optimizations\n",
    "    print(\"\\nLoading data with multi-threading optimizations:\")\n",
    "    load_data_to_database_parallel(data_to_insert, batch_size=1000, num_threads=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
