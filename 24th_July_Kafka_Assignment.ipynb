{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a95552",
   "metadata": {},
   "source": [
    "### Kafka Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4f6f2",
   "metadata": {},
   "source": [
    "1. Setting up a Kafka Producer:\n",
    "a) Write a Python program to create a Kafka producer.\n",
    "b) Configure the producer to connect to a Kafka cluster.\n",
    "c) Implement logic to send messages to a Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) Write a Python program to create a Kafka producer.\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Message delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to topic {msg.topic()} partition {msg.partition()} offset {msg.offset()}\")\n",
    "\n",
    "def create_kafka_producer():\n",
    "    # Configure Kafka producer properties\n",
    "    producer_config = {\n",
    "        'bootstrap.servers': 'your_kafka_broker_1, your_kafka_broker_2',  # Replace with your Kafka brokers\n",
    "        'client.id': 'my_kafka_producer',\n",
    "        'acks': 'all',  # Wait for leader to write to all replicas before acknowledging\n",
    "        'compression.type': 'gzip',  # Optional: Use gzip compression for messages\n",
    "        'batch.num.messages': 1000,  # Optional: Number of messages to batch before sending\n",
    "    }\n",
    "\n",
    "    # Create a Kafka producer instance\n",
    "    producer = Producer(producer_config)\n",
    "\n",
    "    try:\n",
    "        # Define the Kafka topic to send messages to\n",
    "        topic = 'your_kafka_topic'  # Replace with your Kafka topic name\n",
    "\n",
    "        # Send a message to the Kafka topic\n",
    "        message_value = \"Hello, Kafka!\"\n",
    "        producer.produce(topic, value=message_value.encode('utf-8'), callback=delivery_report)\n",
    "\n",
    "        # Flush any remaining messages in the producer\n",
    "        producer.flush()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while sending a message: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the Kafka producer\n",
    "        producer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_kafka_producer()\n",
    "    \n",
    "    \n",
    "# b) Configure the producer to connect to a Kafka cluster.\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Message delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to topic {msg.topic()} partition {msg.partition()} offset {msg.offset()}\")\n",
    "\n",
    "def create_kafka_producer():\n",
    "    # Configure Kafka producer properties\n",
    "    producer_config = {\n",
    "        'bootstrap.servers': 'your_kafka_broker_1:9092, your_kafka_broker_2:9092',  # Replace with your Kafka brokers\n",
    "        'client.id': 'my_kafka_producer',\n",
    "        'acks': 'all',  # Wait for leader to write to all replicas before acknowledging\n",
    "        'compression.type': 'gzip',  # Optional: Use gzip compression for messages\n",
    "        'batch.num.messages': 1000,  # Optional: Number of messages to batch before sending\n",
    "    }\n",
    "\n",
    "    # Create a Kafka producer instance\n",
    "    producer = Producer(producer_config)\n",
    "\n",
    "    try:\n",
    "        # Define the Kafka topic to send messages to\n",
    "        topic = 'your_kafka_topic'  # Replace with your Kafka topic name\n",
    "\n",
    "        # Send a message to the Kafka topic\n",
    "        message_value = \"Hello, Kafka!\"\n",
    "        producer.produce(topic, value=message_value.encode('utf-8'), callback=delivery_report)\n",
    "\n",
    "        # Flush any remaining messages in the producer\n",
    "        producer.flush()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while sending a message: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the Kafka producer\n",
    "        producer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_kafka_producer()\n",
    "\n",
    "# c) Implement logic to send messages to a Kafka topic.\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Message delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to topic {msg.topic()} partition {msg.partition()} offset {msg.offset()}\")\n",
    "\n",
    "def send_messages_to_kafka_topic(num_messages):\n",
    "    # Configure Kafka producer properties\n",
    "    producer_config = {\n",
    "        'bootstrap.servers': 'your_kafka_broker_1:9092, your_kafka_broker_2:9092',  # Replace with your Kafka brokers\n",
    "        'client.id': 'my_kafka_producer',\n",
    "        'acks': 'all',  # Wait for leader to write to all replicas before acknowledging\n",
    "        'compression.type': 'gzip',  # Optional: Use gzip compression for messages\n",
    "        'batch.num.messages': 1000,  # Optional: Number of messages to batch before sending\n",
    "    }\n",
    "\n",
    "    # Create a Kafka producer instance\n",
    "    producer = Producer(producer_config)\n",
    "\n",
    "    try:\n",
    "        # Define the Kafka topic to send messages to\n",
    "        topic = 'your_kafka_topic'  # Replace with your Kafka topic name\n",
    "\n",
    "        # Send messages to the Kafka topic\n",
    "        for i in range(num_messages):\n",
    "            message_value = f\"Message {i+1}\"\n",
    "            producer.produce(topic, value=message_value.encode('utf-8'), callback=delivery_report)\n",
    "\n",
    "        # Flush any remaining messages in the producer\n",
    "        producer.flush()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while sending messages: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the Kafka producer\n",
    "        producer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_messages_to_send = 10  # Number of messages to send\n",
    "    send_messages_to_kafka_topic(num_messages_to_send)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6d744",
   "metadata": {},
   "source": [
    "2. Setting up a Kafka Consumer:\n",
    "   a) Write a Python program to create a Kafka consumer.\n",
    "   b) Configure the consumer to connect to a Kafka cluster.\n",
    "   c) Implement logic to consume messages from a Kafka topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Write a Python program to create a Kafka consumer.\n",
    "\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "def consume_messages_from_kafka_topic():\n",
    "    # Configure Kafka consumer properties\n",
    "    consumer_config = {\n",
    "        'bootstrap.servers': 'your_kafka_broker_1:9092, your_kafka_broker_2:9092',  # Replace with your Kafka brokers\n",
    "        'group.id': 'my_kafka_consumer_group',\n",
    "        'auto.offset.reset': 'earliest',  # Start consuming from the beginning of the topic\n",
    "    }\n",
    "\n",
    "    # Create a Kafka consumer instance\n",
    "    consumer = Consumer(consumer_config)\n",
    "\n",
    "    # Subscribe to the Kafka topic\n",
    "    topic = 'your_kafka_topic'  # Replace with your Kafka topic name\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    try:\n",
    "        # Poll for new messages\n",
    "        while True:\n",
    "            message = consumer.poll(timeout=1.0)\n",
    "\n",
    "            if message is None:\n",
    "                continue\n",
    "            elif not message.error():\n",
    "                print(f\"Received message: {message.value().decode('utf-8')}\")\n",
    "            elif message.error().code() == KafkaError._PARTITION_EOF:\n",
    "                print(f\"Reached end of partition {message.partition()}\")\n",
    "            else:\n",
    "                print(f\"Error while consuming message: {message.error().str()}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Consumer interrupted. Closing consumer.\")\n",
    "        consumer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consume_messages_from_kafka_topic()\n",
    "    \n",
    "    \n",
    "# b) Configure the consumer to connect to a Kafka cluster.\n",
    "\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "def consume_messages_from_kafka_topic():\n",
    "    # Configure Kafka consumer properties\n",
    "    consumer_config = {\n",
    "        'bootstrap.servers': 'your_kafka_broker_1:9092, your_kafka_broker_2:9092',  # Replace with your Kafka brokers\n",
    "        'group.id': 'my_kafka_consumer_group',\n",
    "        'auto.offset.reset': 'earliest',  # Start consuming from the beginning of the topic\n",
    "    }\n",
    "\n",
    "    # Create a Kafka consumer instance\n",
    "    consumer = Consumer(consumer_config)\n",
    "\n",
    "    # Subscribe to the Kafka topic\n",
    "    topic = 'your_kafka_topic'  # Replace with your Kafka topic name\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    try:\n",
    "        # Poll for new messages\n",
    "        while True:\n",
    "            message = consumer.poll(timeout=1.0)\n",
    "\n",
    "            if message is None:\n",
    "                continue\n",
    "            elif not message.error():\n",
    "                print(f\"Received message: {message.value().decode('utf-8')}\")\n",
    "            elif message.error().code() == KafkaError._PARTITION_EOF:\n",
    "                print(f\"Reached end of partition {message.partition()}\")\n",
    "            else:\n",
    "                print(f\"Error while consuming message: {message.error().str()}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Consumer interrupted. Closing consumer.\")\n",
    "        consumer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consume_messages_from_kafka_topic()\n",
    "\n",
    "    \n",
    "# c) Implement logic to consume messages from a Kafka topic.\n",
    "\n",
    "\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "def consume_messages_from_kafka_topic():\n",
    "    # Configure Kafka consumer properties\n",
    "    consumer_config = {\n",
    "        'bootstrap.servers': 'your_kafka_broker_1:9092, your_kafka_broker_2:9092',  # Replace with your Kafka brokers\n",
    "        'group.id': 'my_kafka_consumer_group',\n",
    "        'auto.offset.reset': 'earliest',  # Start consuming from the beginning of the topic\n",
    "    }\n",
    "\n",
    "    # Create a Kafka consumer instance\n",
    "    consumer = Consumer(consumer_config)\n",
    "\n",
    "    # Subscribe to the Kafka topic\n",
    "    topic = 'your_kafka_topic'  # Replace with your Kafka topic name\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    try:\n",
    "        # Poll for new messages\n",
    "        while True:\n",
    "            message = consumer.poll(timeout=1.0)\n",
    "\n",
    "            if message is None:\n",
    "                continue\n",
    "            elif not message.error():\n",
    "                # Process the received message\n",
    "                process_message(message.value().decode('utf-8'))\n",
    "            elif message.error().code() == KafkaError._PARTITION_EOF:\n",
    "                print(f\"Reached end of partition {message.partition()}\")\n",
    "            else:\n",
    "                print(f\"Error while consuming message: {message.error().str()}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Consumer interrupted. Closing consumer.\")\n",
    "        consumer.close()\n",
    "\n",
    "def process_message(message_value):\n",
    "    # Implement your custom logic to process the received message here\n",
    "    print(f\"Received message: {message_value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consume_messages_from_kafka_topic()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77afbee",
   "metadata": {},
   "source": [
    "3. Creating and Managing Kafka Topics:\n",
    "   a) Write a Python program to create a new Kafka topic.\n",
    "   b) Implement functionality to list existing topics.\n",
    "   c) Develop logic to delete an existing Kafka topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Write a Python program to create a new Kafka topic.\n",
    "\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "def create_kafka_topic():\n",
    "    # Configure Kafka broker address\n",
    "    bootstrap_servers = 'your_kafka_broker_1:9092, your_kafka_broker_2:9092'  # Replace with your Kafka brokers\n",
    "\n",
    "    # Create the admin client\n",
    "    admin_client = AdminClient({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "    # Define the properties of the new topic\n",
    "    topic_name = 'your_new_topic'  # Replace with your desired topic name\n",
    "    num_partitions = 3  # Number of partitions for the topic\n",
    "    replication_factor = 1  # Replication factor for the topic (number of replicas)\n",
    "\n",
    "    # Create the new topic\n",
    "    topic = NewTopic(topic_name, num_partitions=num_partitions, replication_factor=replication_factor)\n",
    "    futures = admin_client.create_topics([topic])\n",
    "\n",
    "    # Wait for the topic creation to complete and check for any errors\n",
    "    for topic_name, future in futures.items():\n",
    "        try:\n",
    "            future.result()\n",
    "            print(f\"Topic '{topic_name}' created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create topic '{topic_name}': {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_kafka_topic()\n",
    "\n",
    "    \n",
    "#  b) Implement functionality to list existing topics.\n",
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "def list_kafka_topics():\n",
    "    # Configure Kafka broker address\n",
    "    bootstrap_servers = 'your_kafka_broker_1:9092, your_kafka_broker_2:9092'  # Replace with your Kafka brokers\n",
    "\n",
    "    # Create the admin client\n",
    "    admin_client = AdminClient({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "    # List existing topics\n",
    "    topics = admin_client.list_topics().topics\n",
    "\n",
    "    # Print the list of topics\n",
    "    print(\"Existing Kafka topics:\")\n",
    "    for topic_name in topics:\n",
    "        print(topic_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    list_kafka_topics()\n",
    "\n",
    "# c) Develop logic to delete an existing Kafka topic.\n",
    "\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "def delete_kafka_topic(topic_name):\n",
    "    # Configure Kafka broker address\n",
    "    bootstrap_servers = 'your_kafka_broker_1:9092, your_kafka_broker_2:9092'  # Replace with your Kafka brokers\n",
    "\n",
    "    # Create the admin client\n",
    "    admin_client = AdminClient({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "    # Delete the topic\n",
    "    futures = admin_client.delete_topics([topic_name])\n",
    "\n",
    "    # Wait for the topic deletion to complete and check for any errors\n",
    "    for topic_name, future in futures.items():\n",
    "        try:\n",
    "            future.result()\n",
    "            print(f\"Topic '{topic_name}' deleted successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete topic '{topic_name}': {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    topic_name_to_delete = 'your_topic_to_delete'  # Replace with the name of the Kafka topic you want to delete\n",
    "    delete_kafka_topic(topic_name_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866f977",
   "metadata": {},
   "source": [
    "4. Producing and Consuming Messages:\n",
    "   a) Write a Python program to produce messages to a Kafka topic.\n",
    "   b) Implement logic to consume messages from the same Kafka topic.\n",
    "   c) Test the end-to-end flow of message production and consumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d47aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a) Write a Python program to produce messages to a Kafka topic.\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Message delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to topic {msg.topic()} partition {msg.partition()} offset {msg.offset()}\")\n",
    "\n",
    "def produce_messages_to_kafka_topic():\n",
    "    # Configure Kafka producer properties\n",
    "    producer_config = {\n",
    "        'bootstrap.servers': 'your_kafka_broker_1:9092, your_kafka_broker_2:9092',  # Replace with your Kafka brokers\n",
    "        'client.id': 'my_kafka_producer',\n",
    "        'acks': 'all',  # Wait for leader to write to all replicas before acknowledging\n",
    "        'compression.type': 'gzip',  # Optional: Use gzip compression for messages\n",
    "        'batch.num.messages': 1000,  # Optional: Number of messages to batch before sending\n",
    "    }\n",
    "\n",
    "    # Create a Kafka producer instance\n",
    "    producer = Producer(producer_config)\n",
    "\n",
    "    try:\n",
    "        # Define the Kafka topic to send messages to\n",
    "        topic = 'your_kafka_topic'  # Replace with your Kafka topic name\n",
    "\n",
    "        # Send messages to the Kafka topic\n",
    "        for i in range(10):  # Send 10 messages as an example\n",
    "            message_value = f\"Message {i}\"\n",
    "            producer.produce(topic, value=message_value.encode('utf-8'), callback=delivery_report)\n",
    "\n",
    "        # Flush any remaining messages in the producer\n",
    "        producer.flush()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while sending messages: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the Kafka producer\n",
    "        producer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    produce_messages_to_kafka_topic()\n",
    "\n",
    "#   b) Implement logic to consume messages from the same Kafka topic.\n",
    "\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "def consume_messages_from_kafka_topic():\n",
    "    # Configure Kafka consumer properties\n",
    "    consumer_config = {\n",
    "        'bootstrap.servers': 'your_kafka_broker_1:9092, your_kafka_broker_2:9092',  # Replace with your Kafka brokers\n",
    "        'group.id': 'my_kafka_consumer_group',\n",
    "        'auto.offset.reset': 'earliest',  # Start consuming from the beginning of the topic\n",
    "    }\n",
    "\n",
    "    # Create a Kafka consumer instance\n",
    "    consumer = Consumer(consumer_config)\n",
    "\n",
    "    # Subscribe to the Kafka topic\n",
    "    topic = 'your_kafka_topic'  # Replace with your Kafka topic name\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    try:\n",
    "        # Poll for new messages\n",
    "        while True:\n",
    "            message = consumer.poll(timeout=1.0)\n",
    "\n",
    "            if message is None:\n",
    "                continue\n",
    "            elif not message.error():\n",
    "                print(f\"Received message: {message.value().decode('utf-8')}\")\n",
    "            elif message.error().code() == KafkaError._PARTITION_EOF:\n",
    "                print(f\"Reached end of partition {message.partition()}\")\n",
    "            else:\n",
    "                print(f\"Error while consuming message: {message.error().str()}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Consumer interrupted. Closing consumer.\")\n",
    "        consumer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consume_messages_from_kafka_topic()\n",
    "\n",
    "    \n",
    "# c) Test the end-to-end flow of message production and consumption.\n",
    "# Steps to check nd-to-end flow of message production and consumption.\n",
    "\n",
    "# To test the end-to-end flow of message production and consumption, we need to run both the producer and consumer Python scripts simultaneously. The producer will send messages to the Kafka topic, and the consumer will receive and print those messages. Before running the scripts, make sure you have Kafka installed, running, and the topic is created.\n",
    "\n",
    "# Run the Producer:\n",
    "# Save the producer script (as shown in the previous answer) to a Python file, for example, kafka_producer.py. Replace the bootstrap server address and the topic name with your Kafka cluster configuration.\n",
    "\n",
    "# Run the Consumer:\n",
    "# Save the consumer script (also provided in the previous answer) to another Python file, for example, kafka_consumer.py. Replace the bootstrap server address and the topic name with your Kafka cluster configuration.\n",
    "\n",
    "# Open two terminal windows or command prompt sessions. In one of them, run the producer script:\n",
    "\n",
    "# bash Copy code python kafka_producer.py\n",
    "# This will start the producer and begin sending messages to the Kafka topic.\n",
    "\n",
    "# In the second terminal or command prompt session, run the consumer script:\n",
    "# bash Copy code python kafka_consumer.py\n",
    "# This will start the consumer, which will begin to consume and print the messages received from the Kafka topic.\n",
    "\n",
    "# Observe the Output:\n",
    "# You should now see messages being produced by the producer and consumed by the consumer. The producer will deliver the messages to the Kafka topic, and the consumer will print the received messages.\n",
    "\n",
    "# Test Termination:\n",
    "# To stop the producer and consumer, press Ctrl + C in each terminal window. This will gracefully close the producer and consumer.\n",
    "\n",
    "# By running both the producer and consumer scripts together, you can verify the end-to-end flow of message production and consumption in Kafka. Messages sent by the producer will be received by the consumer, demonstrating successful communication between the two components. This testing approach allows you to ensure that the messages are correctly produced, delivered, and consumed within the Kafka cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6654f5d",
   "metadata": {},
   "source": [
    "5. Working with Kafka Consumer Groups:\n",
    "   a) Write a Python program to create a Kafka consumer within a consumer group.\n",
    "   b) Implement logic to handle messages consumed by different consumers within the same group.\n",
    "   c) Observe the behavior of consumer group rebalancing when adding or removing consumers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eeddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a) Write a Python program to create a Kafka consumer within a consumer group.\n",
    "\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "def consume_messages_from_kafka_topic():\n",
    "    # Configure Kafka consumer properties\n",
    "    consumer_config = {\n",
    "        'bootstrap.servers': 'your_kafka_broker_1:9092, your_kafka_broker_2:9092',  # Replace with your Kafka brokers\n",
    "        'group.id': 'my_kafka_consumer_group',  # Consumer group ID\n",
    "        'auto.offset.reset': 'earliest',  # Start consuming from the beginning of the topic\n",
    "    }\n",
    "\n",
    "    # Create a Kafka consumer instance\n",
    "    consumer = Consumer(consumer_config)\n",
    "\n",
    "    # Subscribe to the Kafka topic within the consumer group\n",
    "    topic = 'your_kafka_topic'  # Replace with your Kafka topic name\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    try:\n",
    "        # Poll for new messages\n",
    "        while True:\n",
    "            message = consumer.poll(timeout=1.0)\n",
    "\n",
    "            if message is None:\n",
    "                continue\n",
    "            elif not message.error():\n",
    "                print(f\"Received message: {message.value().decode('utf-8')}\")\n",
    "            elif message.error().code() == KafkaError._PARTITION_EOF:\n",
    "                print(f\"Reached end of partition {message.partition()}\")\n",
    "            else:\n",
    "                print(f\"Error while consuming message: {message.error().str()}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Consumer interrupted. Closing consumer.\")\n",
    "        consumer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consume_messages_from_kafka_topic()\n",
    "    \n",
    "    \n",
    "#  b) Implement logic to handle messages consumed by different consumers within the same group.\n",
    "\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "def consume_messages_from_kafka_topic():\n",
    "    # Configure Kafka consumer properties\n",
    "    consumer_config = {\n",
    "        'bootstrap.servers': 'your_kafka_broker_1:9092, your_kafka_broker_2:9092',  # Replace with your Kafka brokers\n",
    "        'group.id': 'my_kafka_consumer_group',  # Consumer group ID\n",
    "        'auto.offset.reset': 'earliest',  # Start consuming from the beginning of the topic\n",
    "    }\n",
    "\n",
    "    # Create a Kafka consumer instance\n",
    "    consumer = Consumer(consumer_config)\n",
    "\n",
    "    # Subscribe to the Kafka topic within the consumer group\n",
    "    topic = 'your_kafka_topic'  # Replace with your Kafka topic name\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    try:\n",
    "        # Poll for new messages\n",
    "        while True:\n",
    "            message = consumer.poll(timeout=1.0)\n",
    "\n",
    "            if message is None:\n",
    "                continue\n",
    "            elif not message.error():\n",
    "                partition = message.partition()\n",
    "                offset = message.offset()\n",
    "                key = message.key()\n",
    "                value = message.value().decode('utf-8')\n",
    "                print(f\"Consumer in group '{consumer_config['group.id']}' received message: \"\n",
    "                      f\"Partition: {partition}, Offset: {offset}, Key: {key}, Value: {value}\")\n",
    "            elif message.error().code() == KafkaError._PARTITION_EOF:\n",
    "                print(f\"Reached end of partition {message.partition()}\")\n",
    "            else:\n",
    "                print(f\"Error while consuming message: {message.error().str()}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Consumer interrupted. Closing consumer.\")\n",
    "        consumer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consume_messages_from_kafka_topic()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
