{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ab1f6a7",
   "metadata": {},
   "source": [
    "### Spark Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4290f4",
   "metadata": {},
   "source": [
    "1. Working with RDDs:\n",
    "   a) Write a Python program to create an RDD from a local data source.\n",
    "   b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
    "   c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Write a Python program to create an RDD from a local data source.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def create_rdd_from_local_data():\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CreateRDDFromLocalData\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Your local data (example data)\n",
    "    data = [1, 2, 3, 4, 5]\n",
    "\n",
    "    # Create an RDD from the local data\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "    # Print the RDD elements\n",
    "    rdd_elements = rdd.collect()\n",
    "    print(\"RDD Elements:\", rdd_elements)\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_rdd_from_local_data()\n",
    "\n",
    "    \n",
    "# b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def perform_data_processing_tasks():\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DataProcessingTasks\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Example data (list of tuples)\n",
    "    data = [\n",
    "        (\"Alice\", 25),\n",
    "        (\"Bob\", 30),\n",
    "        (\"Charlie\", 22),\n",
    "        (\"David\", 28),\n",
    "        (\"Eva\", 35)\n",
    "    ]\n",
    "\n",
    "    # Create an RDD from the local data\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "    # Transformation: Filter the RDD to get people younger than 30\n",
    "    rdd_filtered = rdd.filter(lambda x: x[1] < 30)\n",
    "\n",
    "    # Transformation: Map the RDD to create a new RDD with names only\n",
    "    rdd_names = rdd.map(lambda x: x[0])\n",
    "\n",
    "    # Transformation: Sort the RDD based on age in descending order\n",
    "    rdd_sorted = rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "    # Action: Count the number of elements in the RDD\n",
    "    count = rdd.count()\n",
    "\n",
    "    # Action: Collect the elements of the RDD as a list\n",
    "    elements = rdd.collect()\n",
    "\n",
    "    # Action: Calculate the sum of ages in the RDD\n",
    "    sum_of_ages = rdd.map(lambda x: x[1]).sum()\n",
    "\n",
    "    # Action: Find the maximum age in the RDD\n",
    "    max_age = rdd.map(lambda x: x[1]).max()\n",
    "\n",
    "    # Action: Print the RDD elements after the transformations\n",
    "    print(\"Filtered RDD (Younger than 30):\", rdd_filtered.collect())\n",
    "    print(\"Names RDD:\", rdd_names.collect())\n",
    "    print(\"Sorted RDD:\", rdd_sorted.collect())\n",
    "\n",
    "    # Print the results of the actions\n",
    "    print(\"Number of elements in RDD:\", count)\n",
    "    print(\"All elements in RDD:\", elements)\n",
    "    print(\"Sum of ages in RDD:\", sum_of_ages)\n",
    "    print(\"Maximum age in RDD:\", max_age)\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    perform_data_processing_tasks()\n",
    "\n",
    "# c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def analyze_and_manipulate_data():\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DataAnalysisAndManipulation\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Example data (list of tuples)\n",
    "    data = [\n",
    "        (\"Alice\", 25),\n",
    "        (\"Bob\", 30),\n",
    "        (\"Charlie\", 22),\n",
    "        (\"David\", 28),\n",
    "        (\"Eva\", 35)\n",
    "    ]\n",
    "\n",
    "    # Create an RDD from the local data\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "    # Transformation: Map the RDD to create a new RDD with age squared\n",
    "    rdd_age_squared = rdd.map(lambda x: (x[0], x[1]**2))\n",
    "\n",
    "    # Transformation: Filter the RDD to get people whose name starts with 'A'\n",
    "    rdd_names_with_a = rdd.filter(lambda x: x[0].startswith('A'))\n",
    "\n",
    "    # Action: Calculate the sum of ages using reduce\n",
    "    total_age = rdd.map(lambda x: x[1]).reduce(lambda x, y: x + y)\n",
    "\n",
    "    # Action: Calculate the average age using aggregate\n",
    "    total_and_count = rdd.aggregate((0, 0), lambda acc, x: (acc[0] + x[1], acc[1] + 1),\n",
    "                                    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
    "    average_age = total_and_count[0] / total_and_count[1]\n",
    "\n",
    "    # Print the results of the transformations and actions\n",
    "    print(\"RDD with age squared:\", rdd_age_squared.collect())\n",
    "    print(\"RDD with names starting with 'A':\", rdd_names_with_a.collect())\n",
    "    print(\"Total age:\", total_age)\n",
    "    print(\"Average age:\", average_age)\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_and_manipulate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30238c",
   "metadata": {},
   "source": [
    "2. Spark DataFrame Operations:\n",
    "   a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
    "   b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
    "   c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def load_csv_into_dataframe(file_path):\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"LoadCSVIntoDataFrame\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Print the schema and first few rows of the DataFrame\n",
    "    df.printSchema()\n",
    "    df.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_csv_file_path' with the actual path to your CSV file\n",
    "    csv_file_path = 'your_csv_file_path'\n",
    "    load_csv_into_dataframe(csv_file_path)\n",
    "\n",
    "#    b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def perform_dataframe_operations(file_path):\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DataFrameOperations\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Filter the DataFrame to get rows where age is greater than 30\n",
    "    df_filtered = df.filter(df['age'] > 30)\n",
    "\n",
    "    # Group the DataFrame by the 'gender' column and calculate the average age per gender\n",
    "    df_grouped = df.groupBy('gender').avg('age')\n",
    "\n",
    "    # Join the DataFrame with another DataFrame on the 'user_id' column\n",
    "    # Assuming you have another DataFrame 'other_df' with 'user_id' and 'address' columns\n",
    "    # other_df = spark.read.csv('other_csv_file_path', header=True, inferSchema=True)\n",
    "    # df_joined = df.join(other_df, on='user_id', how='inner')\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    print(\"Filtered DataFrame:\")\n",
    "    df_filtered.show()\n",
    "\n",
    "    # Print the grouped DataFrame\n",
    "    print(\"Grouped DataFrame:\")\n",
    "    df_grouped.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_csv_file_path' with the actual path to your CSV file\n",
    "    csv_file_path = 'your_csv_file_path'\n",
    "    perform_dataframe_operations(csv_file_path)\n",
    "\n",
    "# c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def apply_spark_sql_queries(file_path):\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SparkSQLQueries\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Register the DataFrame as a temporary SQL table\n",
    "    df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "    # Perform Spark SQL queries on the DataFrame\n",
    "    # Example 1: Get all rows where age is greater than 30\n",
    "    result1 = spark.sql(\"SELECT * FROM employees WHERE age > 30\")\n",
    "\n",
    "    # Example 2: Calculate the average age\n",
    "    result2 = spark.sql(\"SELECT AVG(age) AS average_age FROM employees\")\n",
    "\n",
    "    # Example 3: Group the data by gender and calculate the average age per gender\n",
    "    result3 = spark.sql(\"SELECT gender, AVG(age) AS average_age FROM employees GROUP BY gender\")\n",
    "\n",
    "    # Print the query results\n",
    "    print(\"Query Result 1:\")\n",
    "    result1.show()\n",
    "\n",
    "    print(\"Query Result 2:\")\n",
    "    result2.show()\n",
    "\n",
    "    print(\"Query Result 3:\")\n",
    "    result3.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_csv_file_path' with the actual path to your CSV file\n",
    "    csv_file_path = 'your_csv_file_path'\n",
    "    apply_spark_sql_queries(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcefdf3",
   "metadata": {},
   "source": [
    "3. Spark Streaming:\n",
    "  a) Write a Python program to create a Spark Streaming application.\n",
    "   b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
    "   c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0374802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Write a Python program to create a Spark Streaming application.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "def process_stream(stream):\n",
    "    # Process each RDD in the stream\n",
    "    for rdd in stream:\n",
    "        if not rdd.isEmpty():\n",
    "            print(\"New batch of data received:\")\n",
    "            # Print the data in the current RDD\n",
    "            for record in rdd.collect():\n",
    "                print(record)\n",
    "\n",
    "def create_spark_streaming_app():\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SparkStreamingApp\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Create a StreamingContext with a batch interval of 1 second\n",
    "    ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "    # Create a DStream from a TCP data source (example: localhost:9999)\n",
    "    # Replace 'localhost' and '9999' with your own data source information\n",
    "    stream = ssc.socketTextStream('localhost', 9999)\n",
    "\n",
    "    # Process each batch of data received from the stream\n",
    "    stream.foreachRDD(process_stream)\n",
    "\n",
    "    # Start the Spark Streaming application\n",
    "    ssc.start()\n",
    "\n",
    "    # Wait for the application to terminate\n",
    "    ssc.awaitTermination()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_spark_streaming_app()\n",
    "\n",
    "#   b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
    "\n",
    "# 1.) Using Kafka as the streaming source:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "def process_stream(stream):\n",
    "    # Process each RDD in the stream\n",
    "    for rdd in stream:\n",
    "        if not rdd.isEmpty():\n",
    "            print(\"New batch of data received:\")\n",
    "            # Print the data in the current RDD\n",
    "            for record in rdd.collect():\n",
    "                print(record)\n",
    "\n",
    "def create_spark_streaming_app_kafka():\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SparkStreamingApp-Kafka\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Create a StreamingContext with a batch interval of 1 second\n",
    "    ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "    # Define Kafka parameters\n",
    "    kafka_params = {\n",
    "        \"bootstrap.servers\": \"your_kafka_broker_1:9092,your_kafka_broker_2:9092\",  # Replace with Kafka broker addresses\n",
    "        \"group.id\": \"spark-streaming-consumer-group\",  # Consumer group ID\n",
    "        \"auto.offset.reset\": \"latest\",  # Start consuming from the latest offset\n",
    "    }\n",
    "\n",
    "    # Create a DStream from Kafka topic 'your_topic'\n",
    "    kafka_stream = KafkaUtils.createDirectStream(\n",
    "        ssc,\n",
    "        ['your_topic'],  # Replace with the topic name\n",
    "        kafkaParams=kafka_params\n",
    "    )\n",
    "\n",
    "    # Process each batch of data received from the Kafka stream\n",
    "    kafka_stream.foreachRDD(process_stream)\n",
    "\n",
    "    # Start the Spark Streaming application\n",
    "    ssc.start()\n",
    "\n",
    "    # Wait for the application to terminate\n",
    "    ssc.awaitTermination()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_spark_streaming_app_kafka()\n",
    "\n",
    "    \n",
    "#  2.) Using a socket as the streaming source:\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "def process_stream(stream):\n",
    "    # Process each RDD in the stream\n",
    "    for rdd in stream:\n",
    "        if not rdd.isEmpty():\n",
    "            print(\"New batch of data received:\")\n",
    "            # Print the data in the current RDD\n",
    "            for record in rdd.collect():\n",
    "                print(record)\n",
    "\n",
    "def create_spark_streaming_app_socket():\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SparkStreamingApp-Socket\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Create a StreamingContext with a batch interval of 1 second\n",
    "    ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "    # Create a DStream from the socket source on localhost:9999\n",
    "    socket_stream = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "    # Process each batch of data received from the socket stream\n",
    "    socket_stream.foreachRDD(process_stream)\n",
    "\n",
    "    # Start the Spark Streaming application\n",
    "    ssc.start()\n",
    "\n",
    "    # Wait for the application to terminate\n",
    "    ssc.awaitTermination()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_spark_streaming_app_socket()\n",
    "    \n",
    "    \n",
    "#   c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "def process_stream(stream):\n",
    "    # Process each RDD in the stream\n",
    "    for rdd in stream:\n",
    "        if not rdd.isEmpty():\n",
    "            print(\"New batch of data received:\")\n",
    "            # Print the data in the current RDD\n",
    "            for record in rdd.collect():\n",
    "                print(record)\n",
    "\n",
    "            # Perform streaming transformations and actions\n",
    "            # Example: Filter the data to get records with age greater than 30\n",
    "            rdd_filtered = rdd.filter(lambda record: record.get('age', 0) > 30)\n",
    "\n",
    "            # Example: Group the data by gender and calculate the average age per gender\n",
    "            rdd_grouped = rdd.groupBy(lambda record: record.get('gender', 'Unknown')).mapValues(\n",
    "                lambda records: sum(record.get('age', 0) for record in records) / len(records)\n",
    "            )\n",
    "\n",
    "            # Print the results of streaming transformations and actions\n",
    "            print(\"Filtered RDD:\")\n",
    "            for record in rdd_filtered.collect():\n",
    "                print(record)\n",
    "\n",
    "            print(\"Grouped RDD:\")\n",
    "            for key, value in rdd_grouped.collect():\n",
    "                print(f\"Gender: {key}, Average Age: {value}\")\n",
    "\n",
    "def create_spark_streaming_app_kafka():\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SparkStreamingApp-Kafka\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Create a StreamingContext with a batch interval of 1 second\n",
    "    ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "    # Define Kafka parameters\n",
    "    kafka_params = {\n",
    "        \"bootstrap.servers\": \"your_kafka_broker_1:9092,your_kafka_broker_2:9092\",  # Replace with Kafka broker addresses\n",
    "        \"group.id\": \"spark-streaming-consumer-group\",  # Consumer group ID\n",
    "        \"auto.offset.reset\": \"latest\",  # Start consuming from the latest offset\n",
    "    }\n",
    "\n",
    "    # Create a DStream from Kafka topic 'your_topic'\n",
    "    kafka_stream = KafkaUtils.createDirectStream(\n",
    "        ssc,\n",
    "        ['your_topic'],  # Replace with the topic name\n",
    "        kafkaParams=kafka_params\n",
    "    )\n",
    "\n",
    "    # Process each batch of data received from the Kafka stream\n",
    "    kafka_stream.foreachRDD(process_stream)\n",
    "\n",
    "    # Start the Spark Streaming application\n",
    "    ssc.start()\n",
    "\n",
    "    # Wait for the application to terminate\n",
    "    ssc.awaitTermination()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_spark_streaming_app_kafka()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0eaa36",
   "metadata": {},
   "source": [
    "4. Spark SQL and Data Source Integration:\n",
    "   a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
    "   b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
    "   c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a1f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def connect_spark_with_database():\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SparkWithDatabase\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Replace the following with your database credentials and connection details\n",
    "    db_url = \"jdbc:postgresql://your_postgresql_host:your_port/your_database\"\n",
    "    db_properties = {\n",
    "        \"user\": \"your_username\",\n",
    "        \"password\": \"your_password\",\n",
    "        \"driver\": \"org.postgresql.Driver\"  # For MySQL, use \"com.mysql.jdbc.Driver\"\n",
    "    }\n",
    "\n",
    "    # Define the table name in the database you want to read\n",
    "    table_name = \"your_table_name\"\n",
    "\n",
    "    # Load data from the database table into a DataFrame\n",
    "    df = spark.read.jdbc(url=db_url, table=table_name, properties=db_properties)\n",
    "\n",
    "    # Perform DataFrame operations or analysis as needed\n",
    "    # For example, display the DataFrame content\n",
    "    df.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    connect_spark_with_database()\n",
    "\n",
    "    \n",
    "#   b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def perform_sql_operations():\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SparkSQLOperations\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Replace the following with your database credentials and connection details\n",
    "    db_url = \"jdbc:postgresql://your_postgresql_host:your_port/your_database\"\n",
    "    db_properties = {\n",
    "        \"user\": \"your_username\",\n",
    "        \"password\": \"your_password\",\n",
    "        \"driver\": \"org.postgresql.Driver\"  # For MySQL, use \"com.mysql.jdbc.Driver\"\n",
    "    }\n",
    "\n",
    "    # Define the table name in the database you want to query\n",
    "    table_name = \"your_table_name\"\n",
    "\n",
    "    # Load data from the database table into a DataFrame\n",
    "    df = spark.read.jdbc(url=db_url, table=table_name, properties=db_properties)\n",
    "\n",
    "    # Register the DataFrame as a temporary SQL table\n",
    "    df.createOrReplaceTempView(\"data_table\")\n",
    "\n",
    "    # Perform Spark SQL operations on the data\n",
    "    # Example 1: Select all rows from the table\n",
    "    result1 = spark.sql(\"SELECT * FROM data_table\")\n",
    "\n",
    "    # Example 2: Filter data using SQL WHERE clause\n",
    "    result2 = spark.sql(\"SELECT * FROM data_table WHERE age > 30\")\n",
    "\n",
    "    # Example 3: Calculate the average age\n",
    "    result3 = spark.sql(\"SELECT AVG(age) AS average_age FROM data_table\")\n",
    "\n",
    "    # Print the query results\n",
    "    print(\"Query Result 1:\")\n",
    "    result1.show()\n",
    "\n",
    "    print(\"Query Result 2:\")\n",
    "    result2.show()\n",
    "\n",
    "    print(\"Query Result 3:\")\n",
    "    result3.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    perform_sql_operations()\n",
    "    \n",
    "# c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def read_data_from_s3(s3_path):\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SparkWithS3\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"your_aws_access_key\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"your_aws_secret_key\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Read data from Amazon S3 into a DataFrame\n",
    "    df = spark.read.csv(s3_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Perform DataFrame operations or analysis as needed\n",
    "    df.show()\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_s3_file_path' with the actual S3 path to your CSV file\n",
    "    s3_file_path = 's3a://your_bucket_name/your_s3_file_path'\n",
    "    read_data_from_s3(s3_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad287d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
