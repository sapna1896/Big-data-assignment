{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36780ef",
   "metadata": {},
   "source": [
    "### Hadoop Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0e1c5",
   "metadata": {},
   "source": [
    "### 1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def read_hadoop_config(filename):\n",
    "    try:\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        core_components = []\n",
    "\n",
    "        for property_elem in root.findall('.//property'):\n",
    "            name_elem = property_elem.find('name')\n",
    "            value_elem = property_elem.find('value')\n",
    "            \n",
    "            if name_elem is not None and value_elem is not None:\n",
    "                name = name_elem.text.strip()\n",
    "                value = value_elem.text.strip()\n",
    "\n",
    "                if 'fs.defaultFS' in name or 'dfs.nameservices' in name:\n",
    "                    core_components.append((name, value))\n",
    "\n",
    "        return core_components\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        return []\n",
    "    except ET.ParseError:\n",
    "        print(f\"Error: Invalid XML format in '{filename}'.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hadoop_config_file = \"path/to/your/core-site.xml\"  # Replace with the actual path to your Hadoop configuration file\n",
    "    core_components = read_hadoop_config(hadoop_config_file)\n",
    "\n",
    "    if core_components:\n",
    "        print(\"Core Components of Hadoop:\")\n",
    "        for name, value in core_components:\n",
    "            print(f\"{name}: {value}\")\n",
    "    else:\n",
    "        print(\"No core components found in the Hadoop configuration file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0169870",
   "metadata": {},
   "source": [
    "### 2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9aba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def get_hdfs_directory_size(hdfs_url, hdfs_directory):\n",
    "    try:\n",
    "        hdfs_client = InsecureClient(hdfs_url)\n",
    "        total_size = 0\n",
    "\n",
    "        # Get a list of all files in the directory\n",
    "        files = hdfs_client.list(hdfs_directory, status=True)\n",
    "\n",
    "        for file_info in files:\n",
    "            file_path = file_info['path']\n",
    "            file_size = file_info['length']\n",
    "            total_size += file_size\n",
    "\n",
    "        return total_size\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hdfs_url = \"http://localhost:9870\"  # Replace with the HDFS NameNode URL\n",
    "    hdfs_directory = \"/path/to/hdfs_directory\"  # Replace with the directory you want to calculate the size for\n",
    "\n",
    "    total_size = get_hdfs_directory_size(hdfs_url, hdfs_directory)\n",
    "\n",
    "    if total_size is not None:\n",
    "        print(f\"Total file size in '{hdfs_directory}': {total_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36854c",
   "metadata": {},
   "source": [
    "### 3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "\n",
    "def mapper(text_chunk):\n",
    "    word_count = Counter()\n",
    "    word_pattern = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "    for line in text_chunk:\n",
    "        words = word_pattern.findall(line.lower())\n",
    "        word_count.update(words)\n",
    "\n",
    "    return word_count\n",
    "\n",
    "def reducer(word_counts):\n",
    "    final_word_count = Counter()\n",
    "\n",
    "    for word_count in word_counts:\n",
    "        final_word_count.update(word_count)\n",
    "\n",
    "    return final_word_count\n",
    "\n",
    "def get_top_n_words(file_path, n):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    chunk_size = len(lines) // multiprocessing.cpu_count()\n",
    "    chunks = [lines[i:i+chunk_size] for i in range(0, len(lines), chunk_size)]\n",
    "\n",
    "    pool = multiprocessing.Pool()\n",
    "    word_counts = pool.map(mapper, chunks)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    merged_word_count = reducer(word_counts)\n",
    "    top_n_words = merged_word_count.most_common(n)\n",
    "\n",
    "    return top_n_words\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"large_text_file.txt\"\n",
    "    N = 10\n",
    "\n",
    "    top_words = get_top_n_words(file_path, N)\n",
    "\n",
    "    if top_words:\n",
    "        print(f\"Top {N} most frequent words:\")\n",
    "        for word, count in top_words:\n",
    "            print(f\"{word}: {count}\")\n",
    "    else:\n",
    "        print(\"No data or file not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7008bc6",
   "metadata": {},
   "source": [
    "### 4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80236765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_namenode_health(namenode_url):\n",
    "    try:\n",
    "        response = requests.get(f\"{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\")\n",
    "        response_json = response.json()\n",
    "        state = response_json['beans'][0]['State']\n",
    "        return state\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while connecting to the NameNode: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_datanode_health(datanode_url):\n",
    "    try:\n",
    "        response = requests.get(f\"{datanode_url}/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState\")\n",
    "        response_json = response.json()\n",
    "        live_data_nodes = response_json['beans'][0]['NumLiveDataNodes']\n",
    "        return live_data_nodes\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while connecting to the DataNode: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hadoop_namenode_url = \"http://namenode-hostname:50070\"  # Replace with your Hadoop NameNode URL\n",
    "    hadoop_datanode_url = \"http://datanode-hostname:50075\"  # Replace with your Hadoop DataNode URL\n",
    "\n",
    "    namenode_state = check_namenode_health(hadoop_namenode_url)\n",
    "    if namenode_state is not None:\n",
    "        print(f\"NameNode State: {namenode_state}\")\n",
    "\n",
    "    live_datanodes = check_datanode_health(hadoop_datanode_url)\n",
    "    if live_datanodes is not None:\n",
    "        print(f\"Live DataNodes: {live_datanodes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc3be2",
   "metadata": {},
   "source": [
    "### 5. Develop a Python program that lists all the files and directories in a specific HDFS path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def list_hdfs_path(hdfs_url, hdfs_path):\n",
    "    try:\n",
    "        hdfs_client = InsecureClient(hdfs_url)\n",
    "\n",
    "        # Get a list of all files and directories in the HDFS path\n",
    "        file_status_list = hdfs_client.list(hdfs_path, status=True)\n",
    "\n",
    "        # Print the list of files and directories\n",
    "        print(f\"Files and Directories in HDFS path '{hdfs_path}':\")\n",
    "        for file_status in file_status_list:\n",
    "            file_path = file_status['path']\n",
    "            file_type = \"File\" if file_status['type'] == 'FILE' else \"Directory\"\n",
    "            print(f\"{file_type}: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hdfs_url = \"http://localhost:9870\"  # Replace with the HDFS NameNode URL\n",
    "    hdfs_path = \"/path/to/hdfs_directory\"  # Replace with the specific HDFS path you want to list\n",
    "\n",
    "    list_hdfs_path(hdfs_url, hdfs_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abac3ef",
   "metadata": {},
   "source": [
    "### 6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49364b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_datanodes_info(hadoop_datanode_url):\n",
    "    try:\n",
    "        response = requests.get(f\"{hadoop_datanode_url}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\")\n",
    "        response_json = response.json()\n",
    "        datanode_info = response_json['beans'][0]\n",
    "        return datanode_info\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while connecting to the DataNode: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_storage_utilization(datanode_info):\n",
    "    if datanode_info:\n",
    "        storage_info = datanode_info['StorageInfo']\n",
    "        total_capacity = storage_info['capacity']\n",
    "        remaining_capacity = storage_info['remaining']\n",
    "        used_capacity = total_capacity - remaining_capacity\n",
    "\n",
    "        return total_capacity, used_capacity, remaining_capacity\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hadoop_datanode_url = \"http://datanode-hostname:50075\"  # Replace with your Hadoop DataNode URL\n",
    "\n",
    "    datanode_info = get_datanodes_info(hadoop_datanode_url)\n",
    "\n",
    "    if datanode_info:\n",
    "        total_capacity, used_capacity, remaining_capacity = analyze_storage_utilization(datanode_info)\n",
    "        if total_capacity and used_capacity and remaining_capacity:\n",
    "            print(\"DataNode Storage Utilization:\")\n",
    "            print(f\"Total Capacity: {total_capacity} bytes\")\n",
    "            print(f\"Used Capacity: {used_capacity} bytes\")\n",
    "            print(f\"Remaining Capacity: {remaining_capacity} bytes\")\n",
    "        else:\n",
    "            print(\"Failed to analyze storage utilization.\")\n",
    "    else:\n",
    "        print(\"No data or DataNode not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9b1fd",
   "metadata": {},
   "source": [
    "### 7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "def submit_hadoop_job(resourcemanager_url, job_name, jar_path, main_class, input_path, output_path):\n",
    "    try:\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        payload = {\n",
    "            \"applicationId\": \"my-hadoop-job\",\n",
    "            \"applicationName\": job_name,\n",
    "            \"amContainerSpec\": {\n",
    "                \"commands\": [\n",
    "                    f\"yarn jar {jar_path} {main_class} {input_path} {output_path}\"\n",
    "                ]\n",
    "            },\n",
    "            \"maxAppAttempts\": 1,\n",
    "            \"resource\": {\n",
    "                \"vCores\": 1,\n",
    "                \"memory\": 512\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = requests.post(f\"{resourcemanager_url}/ws/v1/cluster/apps\", json=payload, headers=headers)\n",
    "        response_json = response.json()\n",
    "\n",
    "        if response.status_code == 202:\n",
    "            print(\"Hadoop job submitted successfully.\")\n",
    "            return response_json['applicationId']\n",
    "        else:\n",
    "            print(f\"Failed to submit Hadoop job. Status Code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while connecting to the ResourceManager: {e}\")\n",
    "        return None\n",
    "\n",
    "def monitor_job_progress(resourcemanager_url, application_id):\n",
    "    try:\n",
    "        response = requests.get(f\"{resourcemanager_url}/ws/v1/cluster/apps/{application_id}\")\n",
    "        response_json = response.json()\n",
    "        return response_json['app']['state']\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while monitoring job progress: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_output_files(hdfs_url, output_path, local_output_dir):\n",
    "    try:\n",
    "        hdfs_client = InsecureClient(hdfs_url)\n",
    "        hdfs_client.download(output_path, local_output_dir)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while downloading output files from HDFS: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    resourcemanager_url = \"http://resourcemanager-hostname:8088\"  # Replace with your YARN ResourceManager URL\n",
    "    hdfs_url = \"http://namenode-hostname:9870\"  # Replace with your HDFS NameNode URL\n",
    "\n",
    "    job_name = \"MyHadoopJob\"\n",
    "    jar_path = \"/path/to/hadoop_job.jar\"  # Replace with the path to your Hadoop job JAR file\n",
    "    main_class = \"com.example.HadoopJobMain\"  # Replace with the main class of your Hadoop job\n",
    "    input_path = \"/path/to/input_data\"  # Replace with the HDFS path of your input data\n",
    "    output_path = \"/path/to/output_data\"  # Replace with the HDFS path where you want to save the output\n",
    "\n",
    "    application_id = submit_hadoop_job(resourcemanager_url, job_name, jar_path, main_class, input_path, output_path)\n",
    "\n",
    "    if application_id:\n",
    "        while True:\n",
    "            job_state = monitor_job_progress(resourcemanager_url, application_id)\n",
    "            if job_state == \"FINISHED\":\n",
    "                print(\"Hadoop job completed successfully.\")\n",
    "                break\n",
    "            elif job_state in [\"FAILED\", \"KILLED\", \"UNKNOWN\"]:\n",
    "                print(f\"Hadoop job failed or terminated. Job State: {job_state}\")\n",
    "                break\n",
    "\n",
    "        # Assuming the Hadoop job writes output to HDFS, download the output files\n",
    "        download_output_files(hdfs_url, output_path, \"local_output_directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54542f",
   "metadata": {},
   "source": [
    "### 8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resourcemanager_url, job_name, jar_path, main_class, input_path, output_path, num_containers, container_memory, container_vcores):\n",
    "    try:\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        payload = {\n",
    "            \"applicationId\": \"my-hadoop-job\",\n",
    "            \"applicationName\": job_name,\n",
    "            \"amContainerSpec\": {\n",
    "                \"commands\": [\n",
    "                    f\"yarn jar {jar_path} {main_class} {input_path} {output_path}\"\n",
    "                ]\n",
    "            },\n",
    "            \"maxAppAttempts\": 1,\n",
    "            \"resource\": {\n",
    "                \"vCores\": container_vcores,\n",
    "                \"memory\": container_memory\n",
    "            },\n",
    "            \"resourceType\": \"YARN\",\n",
    "            \"queue\": \"default\",\n",
    "            \"containers\": num_containers\n",
    "        }\n",
    "\n",
    "        response = requests.post(f\"{resourcemanager_url}/ws/v1/cluster/apps\", json=payload, headers=headers)\n",
    "        response_json = response.json()\n",
    "\n",
    "        if response.status_code == 202:\n",
    "            print(\"Hadoop job submitted successfully.\")\n",
    "            return response_json['applicationId']\n",
    "        else:\n",
    "            print(f\"Failed to submit Hadoop job. Status Code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while connecting to the ResourceManager: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_job_status(resourcemanager_url, application_id):\n",
    "    try:\n",
    "        response = requests.get(f\"{resourcemanager_url}/ws/v1/cluster/apps/{application_id}\")\n",
    "        response_json = response.json()\n",
    "        return response_json['app']['state']\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while getting job status: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    resourcemanager_url = \"http://resourcemanager-hostname:8088\"  # Replace with your YARN ResourceManager URL\n",
    "\n",
    "    job_name = \"MyHadoopJob\"\n",
    "    jar_path = \"/path/to/hadoop_job.jar\"  # Replace with the path to your Hadoop job JAR file\n",
    "    main_class = \"com.example.HadoopJobMain\"  # Replace with the main class of your Hadoop job\n",
    "    input_path = \"/path/to/input_data\"  # Replace with the HDFS path of your input data\n",
    "    output_path = \"/path/to/output_data\"  # Replace with the HDFS path where you want to save the output\n",
    "    num_containers = 1\n",
    "    container_memory = 1024  # Memory in MB for each container\n",
    "    container_vcores = 1\n",
    "\n",
    "    application_id = submit_hadoop_job(resourcemanager_url, job_name, jar_path, main_class, input_path, output_path, num_containers, container_memory, container_vcores)\n",
    "\n",
    "    if application_id:\n",
    "        while True:\n",
    "            job_status = get_job_status(resourcemanager_url, application_id)\n",
    "            print(f\"Job Status: {job_status}\")\n",
    "\n",
    "            if job_status in [\"FINISHED\", \"FAILED\", \"KILLED\"]:\n",
    "                break\n",
    "\n",
    "            time.sleep(10)  # Wait for 10 seconds before checking the status again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701ca11",
   "metadata": {},
   "source": [
    "### 9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe178354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "def submit_mapreduce_job(input_path, split_size):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        # Submit the MapReduce job with the given input split size\n",
    "        cmd = f\"hadoop jar path/to/hadoop_job.jar com.example.HadoopJobMain {input_path} output -D mapreduce.input.fileinputformat.split.minsize={split_size} -D mapreduce.input.fileinputformat.split.maxsize={split_size}\"\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "        end_time = time.time()\n",
    "\n",
    "        execution_time = end_time - start_time\n",
    "        return execution_time\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error while submitting the MapReduce job: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"/path/to/input_data\"  # Replace with the HDFS path of your input data\n",
    "    split_sizes = [64 * 1024 * 1024, 128 * 1024 * 1024, 256 * 1024 * 1024]  # Different input split sizes in bytes\n",
    "\n",
    "    for split_size in split_sizes:\n",
    "        print(f\"Running job with split size: {split_size} bytes\")\n",
    "        execution_time = submit_mapreduce_job(input_path, split_size)\n",
    "\n",
    "        if execution_time is not None:\n",
    "            print(f\"Execution Time: {execution_time:.2f} seconds\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
